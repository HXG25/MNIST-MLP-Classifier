MNIST Handwritten Digit Recognition (MLP Classifier)

This project focuses on building a Multi-Layer Perceptron (MLP) neural network to classify handwritten digits from the MNIST dataset.
The MNIST dataset contains 70,000 grayscale images of handwritten digits (0â€“9), each of size 28Ã—28 pixels.
The goal is to train a neural network model that can accurately predict the digit represented in each image.

  
ðŸŽ¯ Objective
Train an MLP classifier to recognize handwritten digits.
Evaluate performance using accuracy and loss metrics.
Demonstrate the effectiveness of neural networks for image classification tasks.

  
ðŸ§  Model Architecture
Input Layer: 784 neurons (28Ã—28 flattened image)
Hidden Layers: 1-3 Dense layers (depending on your architecture)
Activation Functions: ReLU / Sigmoid
Output Layer: 10 neurons (one for each digit class), Softmax activation

  
ðŸ“Š Dataset
Property	Value
Name	MNIST
Classes	10 digits (0â€“9)
Image Size	28Ã—28 grayscale
Train Samples	60,000
Test Samples	10,000

  
ðŸ”§ Technologies Used
Component	Tools Used
Language	Python
Deep Learning	TensorFlow / Keras or PyTorch 
Data Handling	NumPy, Pandas
Visualization	Matplotlib
Development	Jupyter Notebook
Version Control	Git & GitHub
  
  
  
ðŸš€ Future Enhancements
Use Convolutional Neural Networks (CNN) for higher accuracy
Deploy the model in a web app using Streamlit / Flask
Convert model to ONNX or TensorFlow Lite for mobile deployment




1. Dataset:
   - Dataset: MNIST handwritten digits (60,000 training images + 10,000 test images).
   - Why: MNIST is a classic benchmark dataset for image classification tasks.
     It is widely used to test machine learning models because of its simplicity and popularity.

2. Architecture:
   - Input layer: 784 neurons (28x28 pixels, flattened).
   - Hidden layers: [128, 64] neurons with ReLU activation.
   - Output layer: 10 neurons (digits 0â€“9) with Softmax activation.
   - Initialization: He initialization for ReLU layers.
   - Optimizer: Vanilla gradient descent (learning rate = 0.01).
   - Batch size: 128.

3. Training setup:
   - Loss function: Cross-entropy.
   - Epochs: 10.
   - Implemented forward and backward propagation manually using NumPy.

4. Results summary:
   - Final test accuracy â‰ˆ 93â€“94%.
   - Training loss steadily decreases across epochs.
   - Accuracy curve shows stable improvement.
   - Confusion matrix indicates errors mostly between visually similar digits (e.g., 4 vs 9, 3 vs 5).

5. Strengths:
   - Built entirely from scratch (no deep learning libraries like TensorFlow/PyTorch).
   - Demonstrates understanding of backpropagation, activation functions, and optimization.
   - Good accuracy for a simple MLP model.

6. Limitations:
   - Uses simple gradient descent, no advanced optimizers like Adam.
   - Model training can be slow with NumPy only (CPU).
   - CNNs generally outperform MLPs on MNIST (>98% accuracy).

---------------------------------------------------------------
Below is the full implementation with:
 - Training/Testing loop
 - Loss & accuracy curves
 - Confusion matrix
 - Example predictions
 - Inline comments for clarity
---------------------------------------------------------------
